一、需求文档（PRD）
1. 项目背景与目标

背景
用户（学生）参加全英文线下讲座，英文水平有限，需要：

现场看到英文 + 中文双语实时字幕；

讲座结束后：

下载整场 英文原稿；

下载 中英对照稿；

拿到一份 AI 自动总结稿（中文为主，可附英文提纲）。

技术前提：后端用 Python，ASR 用开源 Whisper（可本地推理），翻译走百度翻译 API，整理/总结走 DeepSeek；前端利用 AI 生成中英文对照稿，后端负责生成 Markdown/Word/PDF 导出。

产品目标

帮助用户在英语环境下「听得懂」「记得住」。

降低语言门槛，不改变老师授课方式。

后端提供统一 API，未来可以接：Web、手机 App、小程序、桌面端。

2. 角色定义

普通用户（学生）

发起/加入一场讲座会话；

看到实时字幕；

讲后下载稿件和总结。

已授权用户（有限登录）

 通过简易登录进入系统；不开放自助注册，账号由管理员预置；获取 Bearer Token；退出登录。

系统管理员（运维/开发）

管理用户、查看服务状态、监控日志。

（可选）讲师账号

管理自己的讲座记录（如果将来要产品化）。

3. 使用场景 / 用户故事

场景一：现场听课

我坐在教室里，用手机打开客户端；

点「开始记录」，手机麦克风录入教授声音；

屏幕上实时出现英文字幕，下方是中文翻译；

延迟在 1–2 秒内，可以跟得上。

场景二：课后复习

下课后，我在 app 里看到这一场讲座；

可以点击「导出英文原稿」「导出中英对照」「查看 AI 总结」；

AI 总结里有：本节主题、关键概念解释、重要例子、Q&A。

场景三：网络不稳定

网络差时，实时字幕可能断断续续；

本地仍在录音，等网络恢复后会把音频补上传，重新生成完整稿子与总结。

4. 业务流程（文字版）

4.1 实时字幕流程

客户端向后端发起 WebSocket 连接，创建/加入一个 lecture 会话；

客户端持续发送音频小片段（如每 0.5–2 秒一帧）；

后端接收音频 → 调用 ASR 服务 → 得到英文文本（附时间戳）；

后端调用翻译服务，把英文翻译成中文；

后端把 {英文, 中文, 时间戳} 推送到该会话的所有订阅客户端；

同时把这些内容写入数据库（或日志队列）。

4.2 讲后总结流程

用户在客户端点「结束讲座」；

后端把这场讲座的所有英文文本按时间拼接，整理成长文档；

后端调用大模型 API，按预设 Prompt 生成多种内容：

中文摘要；

中文要点列表；

可选英文提纲；

把结果存入 DB，并标记讲座状态为「已完成总结」。

4.3 文件导出流程

用户在讲座详情页点击「导出」；

后端根据请求类型生成对应格式：

Transcript_EN.txt / .md

Transcript_EN_ZH.txt / .md

Summary_ZH.md / .pdf

 Word：中英对照或摘要，Python 生成（python-docx）；

 PDF：由 Markdown/HTML 渲染（WeasyPrint/wkhtmltopdf）。

生成完返回下载链接或文件流。

5. 功能需求
5.0 登录与权限

- 提供简易登录接口（账号由管理员创建），返回 Bearer Token；
- 不开放公众注册；
- 所有 API/WS 需带 Token。

5.1 讲座会话管理

创建讲座会话：返回 lecture_id；

加入现有会话（多设备查看同一字幕）；

结束会话：触发「总结任务」。

5.2 实时音频上传

WebSocket 方式：

客户端发送二进制音频帧（如 16kHz mono PCM）；

服务端识别当前用户和 lecture_id；

支持 basic 鉴权（token）；

支持基本心跳检测，连接断开后自动清理会话状态。

5.3 实时语音识别（ASR）

输入：短音频片段（0.5–5 秒）；

输出：英文字符串 + 开始/结束时间戳；

要求：

延迟尽量控制在 1–2 秒；

错误码合理（识别失败、服务不可用）。

方案：本地/自托管 Whisper（小模型优先，支持 GPU/CPU），必要时批量推理。

5.4 实时翻译

输入：一小段英文文本；

输出：对应中文文本；

支持批量翻译（减少请求次数）；

在文本过长时自动分段。

方案：百度翻译 API，带重试和限流保护。

5.5 实时字幕推送

对同一个 lecture_id，所有已连接客户端都能收到相同字幕流；

字幕条目结构：

{
  "type": "subtitle",
  "lecture_id": "xxx",
  "seq": 123,
  "start_ms": 10000,
  "end_ms": 12000,
  "text_en": "This is a sentence.",
  "text_zh": "这是一句话。"
}

5.6 讲后总结与整理

支持自动生成以下内容：

讲座整体摘要（中文主，英文可选）；

关键概念列表；

关键例子、公式、结论；

可选：按时间轴生成 outline。

支持重复触发总结（比如模型更新时重跑）。

方案：DeepSeek 生成摘要/要点/QA/可选提纲，可提示生成示意图描述（前端渲染或留为待绘图）。

5.7 AI 辅助文档生成

- 前端将原文/翻译稿和模型输出整合成中英文对照 Markdown，支持在线预览；
- 支持生成简易示意图占位（如 Mermaid/ASCII 图）供后端/前端展示；
- 后端将 Markdown 渲染成 Word/PDF 并提供下载。

5.8 数据存储与导出

存储粒度：

lecture 元数据；

utterance（每一条字幕片段：时间戳 + 中英文本）；

summary（多种类型：general_summary, outline, qa 等）。

导出格式：

文本：txt / md；

可选：PDF（通过 HTML + 渲染生成）。

Word：python-docx 生成；Markdown 用于在线预览与下载；PDF 由 Markdown/HTML 渲染。

6. 非功能性需求

延迟：从说话到看到字幕 ≤ 2 秒为目标（网络正常情况下）；

并发：初期支持 50–100 个并发讲座，每个讲座 1–3 客户端；

可靠性：

出错不影响录音本身；

实时失败时可以事后重新转写；

安全与隐私：

所有请求走 HTTPS / WSS；

音频和文本按用户隔离，不对外共享；

提供数据删除接口；

 有域名但证书未就绪，部署为手动流程：优先申请正式证书，临时可用自签或内网 HTTP 仅限测试，不能破坏正式 HTTPS/WSS 约束。

运维：

有基础日志（请求、错误、耗时）；

有简单监控项（CPU、内存、ASR 调用失败率等）。
